{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9168669,"sourceType":"datasetVersion","datasetId":5540292},{"sourceId":9177980,"sourceType":"datasetVersion","datasetId":5546972}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/riyaanjain/pneumonia-detector?scriptVersionId=194457702\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Applying Deep Learning Techniques to Diagnose Pneumonia Using Lung X-ray Images\n- By Riyaan Jain, Kashan Ali, and Yash Deswal\n- Please find the corresponding research paper here: https://www.riyaanjainportfolio.com","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-16T09:14:35.991845Z","iopub.execute_input":"2024-08-16T09:14:35.992989Z","iopub.status.idle":"2024-08-16T09:14:42.396845Z","shell.execute_reply.started":"2024-08-16T09:14:35.992949Z","shell.execute_reply":"2024-08-16T09:14:42.395702Z"}}},{"cell_type":"code","source":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\nimport os\nimport pandas as pd\nimport numpy as np\nfrom fastai.metrics import Precision, Recall, F1Score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Background\n- This is our project for altREU: Design, Program, and Use Computers to Benefit Society.\n- Pneumonia is a leading cause of death worldwide, and the number one cause of mortality for children caused by an infectious disease.\n- Chest X-rays are vital in detecting and diagnosing pneumonia, and are one of the most ordered radiological reports, but there is a disproportionate lack of radiology specialists to interpret these X-ray images.\n- We present a tool which implements a convolutional neural network and interprets chest X-ray images and provides a pneumonia diagnosis.\n- Our goal is to provide a tool to assist medical professionals in diagnosing pneumonia using chest X-ray images, which is often difficult to interpret due to the great subtleties in radiological features between healthy and infected lungs.\n\n# Approach\n- We use the Chest X-ray Dataset with Lung Segmentation, found on PhysioNet, which is a large dataset of segmented X-ray images derived from the MIMIC-CXR-JPG dataset.\n- The program will take a lung X-ray image of a patient as input and output whether that patient has pneumonia and the degree to which it is certain of its decision.\n- To serve its purpose as an assisstive tool to medical professionals, the program also outputs a heatmap highlighting the regions of interest to the model in its diagnosis, implemented using Gradient-weighted Class Activation Mapping (Grad-CAM).\n- Upon experimenting with implementations built using TensorFlow and PyTorch, we found that fastai on Python is the most intuitive and powerful tool for our project.","metadata":{}},{"cell_type":"code","source":"# The dataset is randomly split into train, test, and validation folders by a sorting algorithm, and then uploaded to kaggle.\n# It is split into 80:10:10 for train, test, and validation. There are 35k images in total.\n# Within each of train, test, and val, there are already two folders consisting of non-pneumonia and pneumonia X-ray images.\n\ntrain_path = Path('/kaggle/input/full-dataset/kaggle/working/train')\nvalid_path = Path('/kaggle/input/full-dataset/kaggle/working/val')\ntest_path = Path('/kaggle/input/full-dataset/kaggle/working/test')\n\n# Creates a DataLoader object using the pre-split train and validation datasets.\n\ndata = ImageDataLoaders.from_folder(\n    train_path.parent,\n    train = 'train', \n    valid = 'val', \n    item_tfms = Resize(224), \n    batch_tfms = aug_transforms(), \n    num_workers = 4\n)\n\n# Loads metrics\n\nprecision = Precision()\nrecall = Recall()\nf1 = F1Score()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prints the classes to verify.\n\nprint(data.vocab)\nprint(f\"Number of classes: {len(data.vocab)}\")\nprint(f\"Number of output categories: {data.c}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a learner object with ResNet50 model.\n# We chose ResNet50 for its higher accuracy compared to VGG16, but quicker training and fine-tuning times\n# compared to Nick Muchi's model found on the HuggingFace website (see below).\n\n# learn = vision_learner(\n#     data, \n#     resnet50,\n#     metrics = [accuracy, precision, recall, f1],\n#     path=Path(\".\"), \n#     model_dir=Path(\"/kaggle/working/models\")\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Nick Muchi's model from the HuggingFace website. It took too long to train, \n# approximately 3x longer per epoch than ResNet50.\n# Due to kaggle's 12 hour session time limit, this was not feasible.\n# This model reports higher accuracy than ResNet50, a better outcome we would have ideally implemented \n# if we had our own powerful and capable system.\n\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nimport torch.nn as nn\nfrom fastai.vision.all import Learner, accuracy, Path\n\nprocessor = AutoImageProcessor.from_pretrained(\"nickmuchi/vit-finetuned-chest-xray-pneumonia\")\nmodel = AutoModelForImageClassification.from_pretrained(\"nickmuchi/vit-finetuned-chest-xray-pneumonia\")\n\ndef custom_loss_fn(outputs, labels):\n    return nn.CrossEntropyLoss()(outputs.logits, labels)\n\ndef custom_accuracy(outputs, labels):\n    return accuracy(outputs.logits, labels)\n\nlearn = Learner(\n    dls=data,\n    model=model,\n    loss_func=custom_loss_fn,\n    metrics=custom_accuracy,\n    path=Path(\".\"), \n    model_dir=Path(\"/kaggle/working/models\")\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add early stopping when validation loss doesn't improve for 3 epochs.\n\nearly_stopping_cb = EarlyStoppingCallback(monitor='valid_loss', patience=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finds learning rates and prints plot of loss vs learning rate.\n\nlrs = learn.lr_find(suggest_funcs=(minimum, steep, valley))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model.\n\nlearn.save('/kaggle/working/models/learning_rate_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model.\n\nlearn.load('/kaggle/working/models/learning_rate_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model with the valley learning rate. \n# Reduced number of epochs for initial training to 2 epochs due to kaggle's 12 hour session time limit.\n\n#learn.fit_one_cycle(2, lrs.valley, cbs=early_stopping_cb)\n\n# This learning rate below has higher accuracy, but takes 3x as long per epoch. \n# This learning rate represents a small range around the steepest point at the minimum value.\n\nlearn.fit_one_cycle(4, slice(lrs.minimum, lrs.steep), cbs=early_stopping_cb)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save('/kaggle/working/models/trained_model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model.\n\nlearn.load('/kaggle/working/models/learning_rate_model')\n\n# Fine-tune the model. Again, reduced from 20 epochs down to only 2 due to time-limit restrictions.\n\n#learn.fit_one_cycle(2, lrs.valley, cbs=early_stopping_cb)\n\n# Ideal implementation below.\n\nlearn.fit_one_cycle(20, slice(lrs.minimum, lrs.steep), cbs=early_stopping_cb)\n\n# Save the model.\n\nlearn.save('/kaggle/working/models/fine_tuned_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model.\n\nlearn.load('/kaggle/working/models/fine_tuned_model')\n\n# Unfreeze the model and find a new learning rate (hyperparameter tuning).\n\nlearn.unfreeze()\nlrs_finetune = learn.lr_find(suggest_funcs=(minimum, steep, valley))\n\n# Save the model.\n\nlearn.save('/kaggle/working/models/new_learning_rate_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model.\n\nlearn.load('/kaggle/working/models/new_learning_rate_model')\n\n# Continue fine-tuning with the new learning rates.\n\n#learn.fit_one_cycle(2, lrs_finetune.valley, cbs=early_stopping_cb)\n\n# Ideal implementation below.\n\nlearn.fit_one_cycle(10, slice(lrs.minimum, lrs.steep), cbs=early_stopping_cb)\n\n# Save the model.\n\nlearn.save('/kaggle/working/models/new_fine_tuned_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model.\n\nlearn.load('/kaggle/working/models/new_fine_tuned_model')\n\n# Plot the learning curve. Steps vs Loss.\n\nlearn.recorder.plot_loss(with_valid=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a confusion matrix.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the test dataset.\n\ntest_dl = learn.dls.test_dl(get_image_files(test_path), with_labels=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate accuracy on the test dataset\n\ntest_accuracy = learn.validate(dl=test_dl)[1]\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict diagnosis on a single image from the test set with confidence percentage.\n\nimg = PILImage.create('/kaggle/input/full-dataset/kaggle/working/val/pneumonia/0175a8d3-997212d8-5bff4e5b-a748a75c-86eb2fa6.jpg')\n\nprediction, _, probabilities = learn.predict(img)\n\npredicted_class = prediction\nconfidence = probabilities.max().item()\n\nprint(f\"Prediction: {predicted_class}, Certainty: {confidence:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display heatmap of the areas of the image the CNN focused on most when determining decision.\n\nfrom fastai.vision.all import *\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef get_gradcam_heatmap(img_tensor, learn, class_idx=None, layer_idx=-2):\n    m = learn.model.eval()\n    device = next(m.parameters()).device\n    img_tensor = img_tensor.to(device).unsqueeze(0)\n    \n    activations = []\n    gradients = []\n    \n    def forward_hook(module, input, output):\n        activations.append(output)\n\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0])\n\n    layer = m[layer_idx]\n    forward_hook_handle = layer.register_forward_hook(forward_hook)\n    backward_hook_handle = layer.register_backward_hook(backward_hook)\n    \n    output = m(img_tensor)\n    if class_idx is None:\n        class_idx = torch.argmax(output)\n    m.zero_grad()\n    output[0, class_idx].backward()\n    \n    forward_hook_handle.remove()\n    backward_hook_handle.remove()\n    \n    activations = activations[0].detach().cpu()\n    gradients = gradients[0].detach().cpu()\n    \n    pooled_grads = gradients.mean(dim=[0, 2, 3])\n    for i in range(activations.shape[1]):\n        activations[0, i, :, :] *= pooled_grads[i]\n\n    heatmap = activations.mean(dim=1).squeeze()\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= heatmap.max()\n    return heatmap\n\ndef display_gradcam(img_path, learn, class_idx=None, layer_idx=-2):\n    img = PILImage.create(img_path)\n    \n    img_tensor = learn.dls.test_dl([img]).one_batch()[0][0]  #apply test_dl transformations.\n    \n    heatmap = get_gradcam_heatmap(img_tensor, learn, class_idx, layer_idx)\n\n    #convert the image tensor to a NumPy array for visualization.\n    img_np = np.array(img_tensor.permute(1, 2, 0))\n    img_np = img_np - img_np.min()\n    img_np = img_np / img_np.max()\n\n    #resize and apply a colour map to the heatmap.\n    heatmap = cv2.resize(heatmap.numpy(), (img_np.shape[1], img_np.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    \n    #superimpose the heatmap on the original image.\n    superimposed_img = cv2.addWeighted(cv2.cvtColor(np.uint8(255 * img_np), cv2.COLOR_RGB2BGR), 0.6, heatmap, 0.4, 0)\n\n    #display the original image and the Grad-CAM heatmap.\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(img_np)\n    plt.title('Original Image')\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n    plt.title('Grad-CAM')\n    plt.axis('off')\n    \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test image using Grad-CAM.\n\nimg_path = '/kaggle/input/full-dataset/kaggle/working/val/pneumonia/0175a8d3-997212d8-5bff4e5b-a748a75c-86eb2fa6.jpg'\ndisplay_gradcam(img_path, learn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}